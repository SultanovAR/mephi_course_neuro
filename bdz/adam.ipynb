{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pylab as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.050634</td>\n",
       "      <td>0.027240</td>\n",
       "      <td>-0.022914</td>\n",
       "      <td>0.037219</td>\n",
       "      <td>0.018378</td>\n",
       "      <td>-0.032899</td>\n",
       "      <td>0.038020</td>\n",
       "      <td>0.007106</td>\n",
       "      <td>-0.014699</td>\n",
       "      <td>-0.013311</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004743</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>-0.005257</td>\n",
       "      <td>-0.006020</td>\n",
       "      <td>-0.005111</td>\n",
       "      <td>-0.010556</td>\n",
       "      <td>-0.002886</td>\n",
       "      <td>-0.004013</td>\n",
       "      <td>-0.002646</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.082828</td>\n",
       "      <td>0.112354</td>\n",
       "      <td>0.009881</td>\n",
       "      <td>0.012665</td>\n",
       "      <td>0.008541</td>\n",
       "      <td>-0.003085</td>\n",
       "      <td>-0.008792</td>\n",
       "      <td>-0.051937</td>\n",
       "      <td>0.034442</td>\n",
       "      <td>0.018615</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021127</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>0.002466</td>\n",
       "      <td>-0.006346</td>\n",
       "      <td>-0.005416</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>-0.001875</td>\n",
       "      <td>-0.005464</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.042588</td>\n",
       "      <td>0.001930</td>\n",
       "      <td>0.008672</td>\n",
       "      <td>-0.049124</td>\n",
       "      <td>-0.006528</td>\n",
       "      <td>-0.016737</td>\n",
       "      <td>-0.026704</td>\n",
       "      <td>-0.066970</td>\n",
       "      <td>-0.001077</td>\n",
       "      <td>0.058637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005165</td>\n",
       "      <td>-0.002685</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>-0.000939</td>\n",
       "      <td>-0.002155</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>-0.000085</td>\n",
       "      <td>-0.001146</td>\n",
       "      <td>-0.007280</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.042608</td>\n",
       "      <td>0.001906</td>\n",
       "      <td>0.008685</td>\n",
       "      <td>-0.049140</td>\n",
       "      <td>-0.006525</td>\n",
       "      <td>-0.016723</td>\n",
       "      <td>-0.026691</td>\n",
       "      <td>-0.066956</td>\n",
       "      <td>-0.001069</td>\n",
       "      <td>0.058653</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005121</td>\n",
       "      <td>-0.002631</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>-0.001003</td>\n",
       "      <td>-0.002150</td>\n",
       "      <td>0.004290</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.001166</td>\n",
       "      <td>-0.007277</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.005453</td>\n",
       "      <td>-0.084841</td>\n",
       "      <td>-0.021381</td>\n",
       "      <td>-0.010569</td>\n",
       "      <td>0.012817</td>\n",
       "      <td>0.006240</td>\n",
       "      <td>-0.043900</td>\n",
       "      <td>0.008502</td>\n",
       "      <td>-0.095431</td>\n",
       "      <td>-0.005969</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005556</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>-0.010339</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>-0.001127</td>\n",
       "      <td>-0.004829</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>-0.010629</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.050634  0.027240 -0.022914  0.037219  0.018378 -0.032899  0.038020   \n",
       "1 -0.082828  0.112354  0.009881  0.012665  0.008541 -0.003085 -0.008792   \n",
       "2 -0.042588  0.001930  0.008672 -0.049124 -0.006528 -0.016737 -0.026704   \n",
       "3 -0.042608  0.001906  0.008685 -0.049140 -0.006525 -0.016723 -0.026691   \n",
       "4 -0.005453 -0.084841 -0.021381 -0.010569  0.012817  0.006240 -0.043900   \n",
       "\n",
       "         7         8         9  ...        41        42        43        44  \\\n",
       "0  0.007106 -0.014699 -0.013311 ... -0.004743  0.000467 -0.005257 -0.006020   \n",
       "1 -0.051937  0.034442  0.018615 ... -0.021127  0.002920  0.002466 -0.006346   \n",
       "2 -0.066970 -0.001077  0.058637 ... -0.005165 -0.002685  0.001238 -0.000939   \n",
       "3 -0.066956 -0.001069  0.058653 ... -0.005121 -0.002631  0.001206 -0.001003   \n",
       "4  0.008502 -0.095431 -0.005969 ... -0.005556 -0.000033 -0.000223 -0.010339   \n",
       "\n",
       "         45        46        47        48        49  50  \n",
       "0 -0.005111 -0.010556 -0.002886 -0.004013 -0.002646   1  \n",
       "1 -0.005416  0.005263  0.001795 -0.001875 -0.005464   1  \n",
       "2 -0.002155  0.004361 -0.000085 -0.001146 -0.007280   1  \n",
       "3 -0.002150  0.004290 -0.000087 -0.001166 -0.007277   1  \n",
       "4  0.001835 -0.001127 -0.004829  0.001877 -0.010629   1  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(filepath_or_buffer='pretreatmented_data.csv', header=None)\n",
    "FEATURES_INDEX = np.arange(50)\n",
    "LABELS_INDEX = 50\n",
    "BATCH_SIZE = 50\n",
    "LOSS_THRESHOLD = 1e-7\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>-0.023177</td>\n",
       "      <td>-0.034199</td>\n",
       "      <td>-0.071174</td>\n",
       "      <td>0.034209</td>\n",
       "      <td>-0.055537</td>\n",
       "      <td>0.050054</td>\n",
       "      <td>0.085029</td>\n",
       "      <td>-0.002683</td>\n",
       "      <td>-0.022024</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000197</td>\n",
       "      <td>-0.003055</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>-0.000235</td>\n",
       "      <td>-0.005397</td>\n",
       "      <td>-0.003612</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.004931</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2709</th>\n",
       "      <td>-0.068698</td>\n",
       "      <td>0.057827</td>\n",
       "      <td>0.062494</td>\n",
       "      <td>-0.120875</td>\n",
       "      <td>-0.031199</td>\n",
       "      <td>-0.126144</td>\n",
       "      <td>-0.001503</td>\n",
       "      <td>-0.027971</td>\n",
       "      <td>0.042212</td>\n",
       "      <td>-0.037903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007117</td>\n",
       "      <td>-0.001194</td>\n",
       "      <td>-0.000628</td>\n",
       "      <td>-0.002963</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.002984</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>-0.000214</td>\n",
       "      <td>-0.001545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2710</th>\n",
       "      <td>-0.022014</td>\n",
       "      <td>-0.018537</td>\n",
       "      <td>-0.052204</td>\n",
       "      <td>-0.010769</td>\n",
       "      <td>-0.049487</td>\n",
       "      <td>0.042335</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>-0.026063</td>\n",
       "      <td>-0.004448</td>\n",
       "      <td>-0.008374</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015350</td>\n",
       "      <td>-0.018111</td>\n",
       "      <td>0.004594</td>\n",
       "      <td>0.017790</td>\n",
       "      <td>-0.003928</td>\n",
       "      <td>0.013328</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2711</th>\n",
       "      <td>-0.027492</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>-0.001082</td>\n",
       "      <td>-0.047973</td>\n",
       "      <td>-0.013806</td>\n",
       "      <td>-0.009351</td>\n",
       "      <td>-0.009311</td>\n",
       "      <td>-0.024605</td>\n",
       "      <td>-0.013044</td>\n",
       "      <td>0.041648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009767</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.005480</td>\n",
       "      <td>-0.005193</td>\n",
       "      <td>-0.005427</td>\n",
       "      <td>-0.004483</td>\n",
       "      <td>-0.003516</td>\n",
       "      <td>-0.003058</td>\n",
       "      <td>0.003073</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2712</th>\n",
       "      <td>-0.053112</td>\n",
       "      <td>0.017851</td>\n",
       "      <td>-0.001592</td>\n",
       "      <td>-0.010851</td>\n",
       "      <td>-0.040524</td>\n",
       "      <td>-0.143514</td>\n",
       "      <td>0.026496</td>\n",
       "      <td>-0.028555</td>\n",
       "      <td>0.030408</td>\n",
       "      <td>0.003770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005770</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>-0.000558</td>\n",
       "      <td>-0.001323</td>\n",
       "      <td>-0.000568</td>\n",
       "      <td>0.003484</td>\n",
       "      <td>-0.000813</td>\n",
       "      <td>-0.002720</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "2708 -0.023177 -0.034199 -0.071174  0.034209 -0.055537  0.050054  0.085029   \n",
       "2709 -0.068698  0.057827  0.062494 -0.120875 -0.031199 -0.126144 -0.001503   \n",
       "2710 -0.022014 -0.018537 -0.052204 -0.010769 -0.049487  0.042335  0.060547   \n",
       "2711 -0.027492 -0.037501 -0.001082 -0.047973 -0.013806 -0.009351 -0.009311   \n",
       "2712 -0.053112  0.017851 -0.001592 -0.010851 -0.040524 -0.143514  0.026496   \n",
       "\n",
       "            7         8         9  ...        41        42        43  \\\n",
       "2708 -0.002683 -0.022024  0.001740 ... -0.000197 -0.003055  0.006381   \n",
       "2709 -0.027971  0.042212 -0.037903 ...  0.007117 -0.001194 -0.000628   \n",
       "2710 -0.026063 -0.004448 -0.008374 ... -0.015350 -0.018111  0.004594   \n",
       "2711 -0.024605 -0.013044  0.041648 ...  0.009767  0.000280  0.005480   \n",
       "2712 -0.028555  0.030408  0.003770 ... -0.005770  0.001425 -0.000558   \n",
       "\n",
       "            44        45        46        47        48        49  50  \n",
       "2708 -0.000235 -0.005397 -0.003612 -0.000170 -0.004931  0.000354   0  \n",
       "2709 -0.002963  0.001091  0.002984  0.002117 -0.000214 -0.001545   0  \n",
       "2710  0.017790 -0.003928  0.013328  0.001513  0.000267  0.002323   0  \n",
       "2711 -0.005193 -0.005427 -0.004483 -0.003516 -0.003058  0.003073   0  \n",
       "2712 -0.001323 -0.000568  0.003484 -0.000813 -0.002720  0.002752   0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_dataset = dataset[dataset[LABELS_INDEX] == 1]\n",
    "nonspam_dataset = dataset[dataset[LABELS_INDEX] == 0]\n",
    "nonspam_dataset.reset_index(drop=True, inplace=True)\n",
    "spam_dataset.reset_index(drop=True, inplace=True)\n",
    "nonspam_dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2621 1311 438 4370 4370\n"
     ]
    }
   ],
   "source": [
    "rand_indxs_spam = np.arange(len(spam_dataset))\n",
    "rand_indxs_nonspam = np.arange(len(nonspam_dataset))\n",
    "np.random.shuffle(rand_indxs_nonspam)\n",
    "np.random.shuffle(rand_indxs_spam)\n",
    "\n",
    "spam_f_threshold = int(len(spam_dataset)*0.6)\n",
    "spam_s_threshold = int(len(spam_dataset)*0.9)\n",
    "\n",
    "nonspam_f_threshold = int(len(nonspam_dataset)*0.6)\n",
    "nonspam_s_threshold = int(len(nonspam_dataset)*0.9)\n",
    "\n",
    "learn_dataset = pd.concat([spam_dataset.iloc[rand_indxs_spam[:spam_f_threshold]],\n",
    "                          nonspam_dataset.iloc[rand_indxs_nonspam[:nonspam_f_threshold]]])\n",
    "\n",
    "valid_dataset = pd.concat([spam_dataset.iloc[rand_indxs_spam[spam_f_threshold:spam_s_threshold]],\n",
    "                          nonspam_dataset.iloc[rand_indxs_nonspam[nonspam_f_threshold:nonspam_s_threshold]]])\n",
    "\n",
    "test_dataset = pd.concat([spam_dataset.iloc[rand_indxs_spam[spam_s_threshold:]],\n",
    "                         nonspam_dataset.iloc[rand_indxs_nonspam[nonspam_s_threshold:]]])\n",
    "\n",
    "print(len(learn_dataset), len(valid_dataset), len(test_dataset), len(learn_dataset)+len(valid_dataset)+len(test_dataset), len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_dataset = np.array(learn_dataset)\n",
    "valid_dataset = np.array(valid_dataset)\n",
    "test_dataset = np.array(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_dataloader = DataLoader(learn_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=len(valid_dataset), shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (lin1): Linear(in_features=50, out_features=20, bias=True)\n",
       "  (lin2): Linear(in_features=20, out_features=10, bias=True)\n",
       "  (lin3): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (lin4): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.lin1 = nn.Linear(50, 20)\n",
    "        self.lin2 = nn.Linear(20, 10)\n",
    "        self.lin3 = nn.Linear(10, 10)\n",
    "        self.lin4 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.lin1(x))\n",
    "        x = F.tanh(self.lin2(x))\n",
    "        x = F.sigmoid(self.lin3(x))\n",
    "        x = F.sigmoid(self.lin4(x))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_load = False\n",
    "if is_load:\n",
    "    net.load_state_dict(torch.load('gdmoment/net_params.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 50])\n",
      "torch.Size([20])\n",
      "torch.Size([10, 20])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1])\n",
      "tensor([-0.1639, -0.1094, -0.1225, -0.3425, -0.1046, -0.0136,  0.4022,\n",
      "        -0.0202,  0.2056,  0.0189, -0.2305, -0.1111, -0.0374,  0.0133,\n",
      "         0.2797,  0.0038, -0.1852, -0.0170,  0.0695,  0.0726])\n"
     ]
    }
   ],
   "source": [
    "#Инициализация весов\n",
    "STD = [(2/(50+20+1))**(1/2), (2/(20+10+1))**(1/2), (32/(10+10+1))**(1/2), (32/(10+1+1))**(1/2)]\n",
    "null_weigth = []\n",
    "STD.reverse()\n",
    "\n",
    "def get_weights(layer):\n",
    "    if (type(layer) == nn.Linear):\n",
    "        cur_std = STD.pop()\n",
    "        null_weigth.append(torch.randn(layer.weight.data.shape)*cur_std)\n",
    "        null_weigth.append(torch.randn(layer.bias.data.shape)*cur_std)\n",
    "\n",
    "net.apply(get_weights)\n",
    "for k in null_weigth:\n",
    "    print(k.shape)\n",
    "print(null_weigth[1])\n",
    "null_weigth.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.1639, -0.1094, -0.1225, -0.3425, -0.1046, -0.0136,  0.4022,\n",
       "        -0.0202,  0.2056,  0.0189, -0.2305, -0.1111, -0.0374,  0.0133,\n",
       "         0.2797,  0.0038, -0.1852, -0.0170,  0.0695,  0.0726])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_null_weigth = list(null_weigth)\n",
    "\n",
    "def init_weigths(layer):\n",
    "    if(type(layer) == nn.Linear):\n",
    "        layer.weight.data = (tmp_null_weigth.pop()).clone()\n",
    "        layer.bias.data = (tmp_null_weigth.pop()).clone()\n",
    "        \n",
    "net.apply(init_weigths)\n",
    "net.lin1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss(size_average=True)\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(state, lr, betas):\n",
    "    beta1, beta2 = betas\n",
    "    exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "    denom = exp_avg_sq.sqrt().add_(1e-8)\n",
    "    bias_correction1 = 1 - beta1 ** state['step']\n",
    "    bias_correction2 = 1 - beta2 ** state['step']\n",
    "    step_size = lr * math.sqrt(bias_correction2) / bias_correction1\n",
    "    return torch.addcdiv(torch.zeros_like(denom), step_size, torch.ones_like(denom), denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1474: UserWarning: Using a target size (torch.Size([50])) that is different to the input size (torch.Size([50, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1474: UserWarning: Using a target size (torch.Size([21])) that is different to the input size (torch.Size([21, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1474: UserWarning: Using a target size (torch.Size([1311])) that is different to the input size (torch.Size([1311, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mpy_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learn_loss_by_rho = []\n",
    "valid_loss_by_rho = []\n",
    "test_loss_by_rho = []\n",
    "epoch_list_by_rho = []\n",
    "\n",
    "lr_l1_by_rho = []\n",
    "lr_l2_by_rho = []\n",
    "lr_l3_by_rho = []\n",
    "lr_l4_by_rho = []\n",
    "\n",
    "lr_mean_by_rho = []\n",
    "lr_min_by_rho = []\n",
    "lr_max_by_rho = []\n",
    "\n",
    "list_of_betas = [(0, 0), (0.5, 0.3), (0.3, 0.5), (0.7, 0.99), (0.99, 0.7), (0.99, 0.99)]\n",
    "\n",
    "for betas in list_of_betas:\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr, betas=betas)\n",
    "    state = optimizer.state\n",
    "\n",
    "    tmp_null_weigth = list(null_weigth)\n",
    "    net.apply(init_weigths)\n",
    "\n",
    "    learn_epoch_loss = []\n",
    "    valid_epoch_loss = []\n",
    "    test_epoch_loss = []\n",
    "    epoch_list = []\n",
    "\n",
    "    lr_l1 = []\n",
    "    lr_l2 = []\n",
    "    lr_l3 = []\n",
    "    lr_l4 = []\n",
    "\n",
    "    lr_mean = []\n",
    "    lr_min = []\n",
    "    lr_max = []\n",
    "\n",
    "    for epoch in range(100):\n",
    "        loss_acc = []\n",
    "        min_acc = 1000\n",
    "        max_acc = -1000\n",
    "        mean_acc = 0\n",
    "\n",
    "        for learn_data in learn_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            features, labels = learn_data[:, FEATURES_INDEX].float(), learn_data[:, LABELS_INDEX].float()\n",
    "            features.requres_grad = True\n",
    "            labels.requres_grad = True\n",
    "            outputs = net(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_acc.append(float(loss.data))\n",
    "        learn_epoch_loss.append(np.mean(loss_acc))\n",
    "\n",
    "        #критерий останова\n",
    "        if (epoch > 10) and\\\n",
    "            (abs(learn_epoch_loss[len(learn_epoch_loss) - 1] - learn_epoch_loss[len(learn_epoch_loss) - 2]) < LOSS_THRESHOLD):\n",
    "                print(learn_epoch_loss[len(learn_epoch_loss) - 1] - learn_epoch_loss[len(learn_epoch_loss) - 2])\n",
    "                print('lr: ', lr, ' breaked on epoch: ', epoch, '\\n')\n",
    "                break\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            epoch_list.append(epoch)\n",
    "            for valid_data in valid_dataloader:\n",
    "                features, labels = valid_data[:, FEATURES_INDEX].float(), valid_data[:, LABELS_INDEX].float()\n",
    "                outputs = net(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_epoch_loss.append(float(loss.data))\n",
    "\n",
    "            for test_data in valid_dataloader:\n",
    "                features, labels = test_data[:, FEATURES_INDEX].float(), test_data[:, LABELS_INDEX].float()\n",
    "                outputs = net(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_epoch_loss.append(float(loss.data))\n",
    "\n",
    "            state_values = list(state.values())\n",
    "            for k in range(len(state_values)):\n",
    "                g_matrix = get_lr(state_values[k], lr, betas)\n",
    "                mean_acc += float(torch.mean(g_matrix))\n",
    "                max_acc = max(max_acc, float(torch.max(g_matrix)))\n",
    "                min_acc = min(min_acc, float(torch.min(g_matrix)))\n",
    "            lr_max.append(max_acc)\n",
    "            lr_mean.append(mean_acc/len(state_values))\n",
    "            lr_min.append(min_acc)\n",
    "\n",
    "            g_matrix = get_lr(state_values[0], lr, betas)\n",
    "            lr_l1.append(float(g_matrix[10, 30]))\n",
    "\n",
    "            g_matrix = get_lr(state_values[2], lr, betas)\n",
    "            lr_l2.append(float(g_matrix[9, 0]))\n",
    "\n",
    "            g_matrix = get_lr(state_values[4], lr, betas)\n",
    "            lr_l3.append(float(g_matrix[5, 5]))\n",
    "\n",
    "            g_matrix = get_lr(state_values[6], lr, betas)\n",
    "            lr_l4.append(float(g_matrix[0, 3]))\n",
    "        \n",
    "    learn_loss_by_rho.append(learn_epoch_loss)\n",
    "    valid_loss_by_rho.append(valid_epoch_loss)\n",
    "    test_loss_by_rho.append(test_epoch_loss)\n",
    "    epoch_list_by_rho.append(epoch_list)\n",
    "\n",
    "    lr_l1_by_rho.append(lr_l1)\n",
    "    lr_l2_by_rho.append(lr_l1)\n",
    "    lr_l3_by_rho.append(lr_l1)\n",
    "    lr_l4_by_rho.append(lr_l1)\n",
    "\n",
    "    lr_mean_by_rho.append(lr_mean)\n",
    "    lr_min_by_rho.append(lr_min)\n",
    "    lr_max_by_rho.append(lr_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 50])\n",
      "torch.Size([20])\n",
      "torch.Size([10, 20])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i in state:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (7,) and (10,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-4c69e3105605>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_betas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_by_rho\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_l1_by_rho\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mu: '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' l1w10,30'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3259\u001b[0m                       mplDeprecation)\n\u001b[1;32m   3260\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3261\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3262\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3263\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1715\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1716\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1717\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 243\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (7,) and (10,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117c4f9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(25,15))\n",
    "plt.subplot(4, 2, 5)\n",
    "for num, i in enumerate(list_of_betas):\n",
    "    plt.plot(epoch_by_rho[num], lr_l1_by_rho[num], label='mu: '+str(i)+' l1w10,30')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(4, 2, 6)\n",
    "for num, i in enumerate(list_of_betas):\n",
    "    plt.plot(epoch_by_rho[num], lr_l2_by_rho[num], label='mu: '+str(i)+' lr l2 w9,0')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(4, 2, 7)\n",
    "for num, i in enumerate(list_of_betas):\n",
    "    plt.plot(epoch_by_rho[num], lr_l3_by_rho[num], label='mu: '+str(i)+' lr l3 w5,5')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(4, 2, 8)\n",
    "for num, i in enumerate(list_of_betas):\n",
    "    plt.plot(epoch_by_rho[num], lr_l4_by_rho[num], label='mu: '+str(i)+' lr l4 w0,3')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "for num, i in enumerate(list_of_betas):\n",
    "    plt.plot(epoch_by_rho[num], lr_max_by_rho[num], label='mu: '+str(i)+' max')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "for num, i in enumerate(list_of_betas):\n",
    "    plt.plot(epoch_by_rho[num], lr_mean_by_rho[num], label='mu: '+str(i)+' mean')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "for num, i in enumerate(list_of_betas):\n",
    "    plt.plot(epoch_by_rho[num], lr_min_by_rho[num], label='mu: '+str(i)+' min')\n",
    "plt.legend()\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('adam/lrs.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "plt.subplot(1, 3, 1)\n",
    "for num, i in enumerate(list_of_betas):\n",
    "    plt.plot(learn_loss_by_rho[num], label='mu: '+str(i))\n",
    "plt.legend()\n",
    "plt.title('learn')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "for num, i in enumerate(list_of_betas):\n",
    "    plt.plot(epoch_by_rho[num], valid_loss_by_rho[num], label='mu: '+str(i))\n",
    "plt.legend()\n",
    "plt.title('valid')\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "for num, i in enumerate(list_of_betas):\n",
    "    plt.plot(epoch_by_rho[num], test_loss_by_rho[num], label='mu: '+str(i))\n",
    "plt.legend()\n",
    "plt.title('test')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('adam/loss.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'adam/net_params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
